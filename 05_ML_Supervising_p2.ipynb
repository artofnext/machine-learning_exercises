{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a K-NN classifier that is specifically suited for the dataset from previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data functions\n",
    "\n",
    "# generates set of all possible bit patterns\n",
    "# returns list of bit pattern lists\n",
    "def generateDataSet(length = 2):\n",
    "    result = []\n",
    "    formatter = \"0\" + str(length) + \"b\"\n",
    "    for i in range(2 ** length):\n",
    "        result.append([int(x) for x in format(i, formatter)])\n",
    "    return result\n",
    "\n",
    "# classification rule odd = -1 even = 1\n",
    "# input list of bit pattern lists\n",
    "# return list\n",
    "def desiredOutputOddEven(inputV):\n",
    "    result = []\n",
    "    for x in inputV:\n",
    "        res = 1 if (int(\"\".join(str(elem) for elem in x), 2) % 2) == 0  else -1\n",
    "        result.append(res)\n",
    "    return result\n",
    "\n",
    "# return 2 sets of randomly selected elements as a sublists of a list \n",
    "def splitDataset(inputV, fraction = .3):\n",
    "    inputLength = len(inputV)\n",
    "    # generate list of random int without duplicates \n",
    "    randIndexes = random.sample(range(inputLength - 1), int(inputLength * fraction))\n",
    "    output1 = []\n",
    "    output2 = []\n",
    "    for index, x in enumerate(inputV):\n",
    "        if index in randIndexes:\n",
    "            output1.append(x[:])\n",
    "        else:\n",
    "            output2.append(x[:])\n",
    "    return [output1, output2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# return Hamming distance\n",
    "def getDistance(pattern1, pattern2):\n",
    "    if (len(pattern1) != len(pattern1)):\n",
    "        print('Patterns length missmatch!')\n",
    "        return False\n",
    "    matchedBits = 0\n",
    "    for index, bit in enumerate(pattern1):\n",
    "        if (bit != pattern2[index]):\n",
    "            matchedBits += 1\n",
    "    return matchedBits\n",
    "\n",
    "# cumulative error\n",
    "def calculateError (vector1, vector2):\n",
    "    error = 0\n",
    "    for index, v1 in enumerate(vector1):\n",
    "        error += 0 if v1 == vector2[index] else 1\n",
    "    return error / len(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-NN classifier\n",
    "class ClassifierKNN:\n",
    "    def __init__(self, inputV, outputV, k = 3):\n",
    "        self.inputV = inputV\n",
    "        self.outputV = outputV\n",
    "        self.k = k\n",
    "        \n",
    "    def clasify(self, sample):\n",
    "        distanseV = []\n",
    "        # calculate distances to each element\n",
    "        for x in self.inputV:\n",
    "            distanseV.append(getDistance(x, sample))\n",
    "        #print (distanseV)\n",
    "        # get the closest k first indexes\n",
    "        indexesOfKClosest = list(np.argsort(distanseV))[:self.k]\n",
    "        #print (indexesOfKClosest)\n",
    "        # get the classes for these elements\n",
    "        outputs = np.array(self.outputV)\n",
    "        classes = list(outputs[indexesOfKClosest])\n",
    "        #print(classes)\n",
    "        # evaluate the dominant class\n",
    "        if sum(classes) > 0 :\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def clasifyDataset(self, sampleV):\n",
    "        outputV = []\n",
    "        for sample in sampleV:\n",
    "            outputV.append(self.clasify(sample))\n",
    "        return (outputV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an experiment for 30 repetitions at different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 30\n",
    "\n",
    "# set of K\n",
    "kV = [3,5,11]\n",
    "\n",
    "errorsV = []\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# generate dataset of 1024 10-bit elements\n",
    "datas1 = generateDataSet(10)\n",
    "\n",
    "for k in kV:\n",
    "    errorSet = []\n",
    "    for i in range(repeats): \n",
    "        # randomly split dataset by 50% / 50%\n",
    "        splitted1 = splitDataset(datas1, 0.5)\n",
    "        # dataset for training\n",
    "        trainInputV = splitted1[1]\n",
    "        # dataset for testing\n",
    "        testInputV = splitted1[0]\n",
    "        # generate desired outputs for train dataset\n",
    "        trainOutputV = desiredOutputOddEven(trainInputV)\n",
    "        # generate desired outputs for test dataset for comparison\n",
    "        testOutputV = desiredOutputOddEven(testInputV)\n",
    "        # instantiate classifier instance\n",
    "        classif1 = ClassifierKNN(trainInputV, trainOutputV, k)\n",
    "        # get classifier output\n",
    "        classifiedOutputV = classif1.clasifyDataset(testInputV)\n",
    "        # compare with desired and calculate error\n",
    "        error = calculateError(testOutputV, classifiedOutputV)\n",
    "        # represent error in 100% scale\n",
    "        errorSet.append(error * 100)\n",
    "    \n",
    "    errorsV.append(errorSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.boxplot((errorsV),labels = kV)\n",
    "plot.title(\"% Errors vs k\")\n",
    "plot.ylabel(\"% of errors\")\n",
    "plot.xlabel(\"K\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph, the number of errors decreases when using a larger K. I used a data separation of 50% / 50%, because with a separation of 70% / 30%, the error at K greater than 3 turned out to be zero (in fact, with the original even-odd function, the classification depends on the state of the last bit, which is a fairly primitive case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculating Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set by bitNumber bit value\n",
    "# return list of lists\n",
    "def splitByBit(inputV, bitNumber = 0):\n",
    "    output1 = []\n",
    "    output2 = []\n",
    "    for x in inputV:\n",
    "        if x[bitNumber] > 0:\n",
    "            output1.append(x[:])\n",
    "        else:\n",
    "            output2.append(x[:])\n",
    "    return [output1, output2]\n",
    "\n",
    "# return probabilipy of p in inputV dataset\n",
    "def probabilipyP(inputV, p):\n",
    "    pV = [i for i in inputV if i == p]\n",
    "    return len(pV) / len(inputV)\n",
    "\n",
    "# return entropy for dataset\n",
    "def entrophy(outputV):\n",
    "    prP1 = probabilipyP(outputV, 1)\n",
    "    prP2 = probabilipyP(outputV, -1)\n",
    "    # avoid log2(0)\n",
    "    entrophy = -((prP1 * (0 if prP1 == 0 else np.log2(prP1)) + (prP2 * (0 if prP2 == 0 else np.log2(prP2)))))\n",
    "    return entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9944132844077624\n",
      "0.9989768567398462\n"
     ]
    }
   ],
   "source": [
    "# testing part\n",
    "dataset2 = generateDataSet(10)\n",
    "splitted2 = splitByBit(dataset2)\n",
    "splitted2 = splitDataset(dataset2, 0.3)\n",
    "datasPart1 = splitted2[0]\n",
    "datasPart2 = splitted2[1]\n",
    "\n",
    "#print(probabilipyP(desiredOutputOddEven(datasPart1), 1))\n",
    "#print(probabilipyP(desiredOutputOddEven(datasPart2), -1))\n",
    "\n",
    "print(entrophy(desiredOutputOddEven(datasPart1)))\n",
    "print(entrophy(desiredOutputOddEven(datasPart2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Information Gain for 2 splits by bit at position \"splitBy\"\n",
    "def gainE(inputV, splitBy):\n",
    "    #split the dataset by feature\n",
    "    splittedV = splitByBit(inputV, splitBy)\n",
    "    # generate outputs for datasets\n",
    "    outputsV = []\n",
    "    for dataset in splittedV:\n",
    "        outputsV.append(desiredOutputOddEven(dataset))\n",
    "    # entrophy for each splitted dataset\n",
    "    entrophyV = []\n",
    "    for dataset in outputsV:\n",
    "        entrophyV.append(entrophy(dataset))\n",
    "    # entrophy for whole dataset\n",
    "    wholeEntrophy = entrophy(desiredOutputOddEven(inputV))\n",
    "    # weighted average of entropy for each dataset\n",
    "    entrophyWA = 0\n",
    "    for index, entr in enumerate(entrophyV):\n",
    "        entrophyWA += entr * len(outputsV[index])/len(inputV)\n",
    "    # information gain\n",
    "    gain = wholeEntrophy - entrophyWA\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the IG for all sets, dividing the initial data set sequentially for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG for dataset divided by 0-th bit is 0.0\n",
      "IG for dataset divided by 1-th bit is 0.0\n",
      "IG for dataset divided by 2-th bit is 0.0\n",
      "IG for dataset divided by 3-th bit is 0.0\n",
      "IG for dataset divided by 4-th bit is 0.0\n",
      "IG for dataset divided by 5-th bit is 0.0\n",
      "IG for dataset divided by 6-th bit is 0.0\n",
      "IG for dataset divided by 7-th bit is 0.0\n",
      "IG for dataset divided by 8-th bit is 0.0\n",
      "IG for dataset divided by 9-th bit is 1.0\n"
     ]
    }
   ],
   "source": [
    "# generate dataset\n",
    "dataset5 = generateDataSet(10)\n",
    "\n",
    "for bit in range(len(dataset5[0])):\n",
    "    ig = gainE(dataset5, bit)\n",
    "    print(\"IG for dataset divided by \" + str(bit) + \"-th bit is \" + str(ig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a perfectly balanced data set (all possible combinations), and the output values depend on the value of the last bit (even-odd function), the resulting IG data are discrete.\n",
    "Now let's take, for example, a part of the entire dataset, randomly selected from the entire set (60%), which will introduce a certain unevenness (with a high probability, this set will be missing some even or odd bit patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG for dataset divided by 0-th bit is 1.1260930968037819e-05\n",
      "IG for dataset divided by 1-th bit is 7.91137040883072e-05\n",
      "IG for dataset divided by 2-th bit is 0.0005169763908419167\n",
      "IG for dataset divided by 3-th bit is 0.0002683148087453402\n",
      "IG for dataset divided by 4-th bit is 0.0007539616606945243\n",
      "IG for dataset divided by 5-th bit is 0.0005701132596442893\n",
      "IG for dataset divided by 6-th bit is 2.5699077752072697e-05\n",
      "IG for dataset divided by 7-th bit is 0.00043797267561052067\n",
      "IG for dataset divided by 8-th bit is 0.00028390597451743993\n",
      "IG for dataset divided by 9-th bit is 0.9987061487157054\n"
     ]
    }
   ],
   "source": [
    "# generate dataset and take only 60%\n",
    "dataset6 = splitDataset(generateDataSet(10), 0.6)[0]\n",
    "\n",
    "for bit in range(len(dataset6[0])):\n",
    "    ig = gainE(dataset6, bit)\n",
    "    print(\"IG for dataset divided by \" + str(bit) + \"-th bit is \" + str(ig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, that dividing the data set on the basis of the last bit gives the maximum value of the IG. IG is a measure of the ordering of data sets divided by a certain feature (as opposed to entropy, which is a measure of disorder in the data). Thus, IG can be used to evaluate the quality of data subdivision. By sequentially dividing the data according to various features and evaluating the IG separation, it is possible to build a so-called \"decision tree\". In our case, this tree has only one fork, since the dataset is classified by 1 feature (the last bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
